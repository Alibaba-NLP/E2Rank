<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Extending a single text embedding model to perform both high-quality retrieval and listwise reranking, offering both computational efficiency and competitive ranking accuracy.">
  <meta name="keywords" content="Text Embedding, Reranking, Information Retrieval, NLP, Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>E2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">E2Rank: Your Text Embedding can Also be an Effective and Efficient
              Listwise Reranker</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://liuqi6777.github.io">Qi Liu</a><sup>1,2</sup>,</span>
              <span class="author-block">
                Yanzhao Zhang<sup>2</sup>,</span>
              <span class="author-block">
                Mingxin Li<sup>2</sup>,
              </span>
              <span class="author-block">
                Dingkun Long<sup>2</sup>,
              </span>
              <span class="author-block">
                Pengjun Xie<sup>2</sup>,
              </span>
              <span class="author-block">
                Jiaxin Mao<sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Renmin University of China,</span>
              <span class="author-block"><sup>2</sup>Tongyi Lab, Alibaba Group</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <!-- <a href="https://arxiv.org/pdf/2510.00000"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2510.22733"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a href="https://github.com/Alibaba-NLP/E2Rank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <!-- Model Link. -->
                  <span class="link-block">
                    <a href="https://huggingface.co/collections/Alibaba-NLP/e2rank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <!-- <i class="far fa-images"></i> -->
                        <p style="font-size:18px">ðŸ¤—</p>
                        <!-- ðŸ”— -->
                      </span>
                      <span>Models</span>
                    </a>
                  </span>
                  <!-- Data Link. -->
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/Alibaba-NLP/E2Rank_ranking_datasets"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser has-text-centered">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="./static/images/cover.png" width="90%" />
        <p style="width: 90%; margin-left: auto; margin-right: auto">
          <b>(a)</b> Overview of E2Rank. <b>(b)</b> Average reranking performance on the BEIR benchmark, E2Rank
          outperforms other baselines. <b>(c)</b> Reranking latency per query on the Covid dataset, E2Rank can achieve
          several times the acceleration compared with RankQwen3.
        </p>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              Text embedding models serve as a fundamental component in real-world search
              applications. However, their ranking fidelity remains limited compared to dedicated rerankers,
              especially recent LLM-based listwise rerankers, which capture fine-grained query-document
              and document-document interactions.
            </p>
            <p>
              In this paper, we propose a simple yet effective unified framework <b>E2Rank</b>,
              means <b>E</b>fficient <b>E</b>mbedding-based <b>Rank</b>ing
              (also means <b>Embedding-to-Rank</b>), which extends a single text embedding model
              to perform both high-quality retrieval and listwise reranking through continued
              training under a listwise ranking objective, thereby achieving strong effectiveness
              with remarkable efficiency. By applying cosine similarity between the query and
              document embeddings as a unified ranking function, the listwise ranking prompt,
              which is constructed from the original query and its candidate documents, serves
              as an enhanced query enriched with signals from the top-K documents, akin to
              <b>pseudo-relevance feedback (PRF)</b> in traditional retrieval models. This design
              preserves the efficiency and representational quality of the base embedding model
              while significantly improving its reranking performance.
            </p>
            <p>
              Empirically, E2Rank achieves <b>state-of-the-art results</b> on the BEIR reranking benchmark
              and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark,
              with very low reranking latency. We also show that E2Rank's advanced embedding ability
              on the MTEB benchmark.
            </p>
            <p><b>
                Our work highlights the potential of single embedding models to serve as unified
                retrieval-reranking engines, offering a practical, efficient, and accurate alternative
                to complex multi-stage ranking systems.
              </b></p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section no_pad_section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">E2Rank: Embedding-to-Rank</h2>
          <div class="content has-text-justified">
            <h3 class="title is-4">Listwise Prompts as Pseudo Relevance Feedback Query</h3>
            <p>
              E2Rank leverages the <span style="color: red"><b>pseudo relevance feedback</b></span>
              signals of the top-K retrieved documents
              to strengthen query representation. Instead of treating each query-document pair
              independently, the listwise prompt is constructed by concatenating the query and its
              candidate documents into a single sequence, allowing the model to capture both
              query-document and document-document interactions within the candidate set.
            </p>
            <p>
              During inference, cosine similarity between the pseudo query embedding (generated from the listwise
              prompt) and each standalone document embedding serves as the ranking function.
              This formulation keeps the model highly efficient while significantly improving
              listwise fidelity.
            </p>
            <h3 class="title is-4">Training the Unified Embedding and Listwise Reranking Model</h3>
            <p>
              The training of E2Rank follows a two-stage paradigm designed to retain strong retrieval
              capability while injecting listwise ranking signals into the embedding space.
            </p>
            <p>
              <b>Stage 1: Training the Embedding with Contrastive Learning</b>
            </p>
            <p>
              We first train the base embedding model with contrastive learning.
              This stage ensures strong initial retrieval accuracy and computational efficiency.
              The model trained in this stage is directly capable of standalone retrieval and
              serves as the initialization for the unified ranking framework.
            </p>
            <p>
              <b>Stage 2: Listwise Ranking Enhancement</b>
            </p>
            <p>
              To further improve the ranking fidelity within the retrieved candidate set,
              we perform continued training using a learning-to-rank objective, RankNet Loss.
              Given a query and its top-K retrieved documents, we construct a listwise prompt
              by concatenating them as pseudo relevance feedback query.
              During training, we optimize cosine similarity between this enhanced
              query embedding and each candidate document embedding to align with ground-truth
              relevance labels. Contrastive learning is also used in this stage.
              <b>This stage injects rich ranking knowledge into the embedding model,
                enabling it to serve not only as a retriever but also as an effective and efficient
                listwise reranker.</b>
            </p>
            <p>
              As a result, E2Rank unifies retrieval and reranking within a single model.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section no_pad_section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified">
            <h3 class="title is-4">Reranking Performance</h3>
            <p>
              <b>Effectiveness.</b> As shown in Figure (b), E2Rank achieves <b>state-of-the-art performance on the
                BEIR
                benchmark</b>.
              It consistently RankQwen3 and approaches, demonstrateing that a single embedding model, when properly
              trained, can perform
              high-quality listwise reranking without relying on heavyweight generative architectures.
            </p>
            <div class="has-text-centered">
              <img src="./static/images/beir_results.png" width="90%" />
            </div>
            <p>
              Additionally, E2Rank delivers strong results on the reasoning-intensive BRIGHT benchmark, without any RL
              training and
              training data synthesis, showcasing its versatility across diverse ranking scenarios.
            </p>
            <div class="has-text-centered">
              <img src="./static/images/bright_results.png" width="90%" />
            </div>
            <p>
              <b>Efficiency.</b> E2Rank inherits the advantages of the embedding model, supports batch inference,
              and can encode document embeddings offline, further reducing online reranking latency.
              As shown in Figure (c), E2Rank significantly reduces inference latency across
              all model sizes compared to RankQwen3, achieving up to about <b>5x speedup at 8B</b>
              while maintaining superior ranking performance.
              Even E2Rank-8B model is faster than RankQwen3-0.6B.
            </p>
            <h3 class="title is-4">Embedding Performance</h3>
            <p>
              Beyond reranking gains, on the MTEB benchmark, E2Rank demonstrates strong embedding capabilities,
              while E2Rank-8B shows slight performance advantages on average compared to previous advanced models.
              Notably, compared with the variant with only contrastive learning, distilling from richer ranking
              signals will bring consistent and significant enhancements in retrieval tasks , demonstrating the
              effectiveness of the ranking objective.
            </p>
            <div class="has-text-centered">
              <img src="./static/images/mteb_results.png" width="90%" />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">BitTeX</h2>
          <pre><code>
            @misc{liu2025e2rank,
              title={E2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker}, 
              author={Qi Liu and Yanzhao Zhang and Mingxin Li and Dingkun Long and Pengjun Xie and Jiaxin Mao},
              year={2025},
              eprint={2510.22733},
              archivePrefix={arXiv},
              primaryClass={cs.CL},
              url={https://arxiv.org/abs/2510.22733}, 
        }
        </code></pre>
        </div>
      </div>
    </div>
  </section>



  <footer class="footer">
    <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a
              rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
    <!-- </div> -->
  </footer>

</body>

</html>